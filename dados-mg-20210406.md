# Automação da catalogação de data packages no CKAN

Gabriel, André e Francisco

## Discussão

- Em termos de inspirações para o processo mais amplo de publicação de dados abertos, o [processo utilizado](https://resources.data.gov/resources/data-gov-open-data-howto/) pelo data.gov é uma fonte de inspiração

    > Under the OPEN Government Data Act and the Open Data Policy, federal agencies are required to publish an enterprise data inventory, provided as a data.json file,using the standard Project Open Data metadata schema. The machine readable listing, as a standalone JSON file on the agency’s website at agency.gov/data.json. This data.json file is what gets harvested to the Data.gov catalog.
    > 
    > Federal agencies that do not have a platform to inventory their metadata can make use of a free service hosted by Data.gov called inventory.data.gov (see the separate guide). Contact the Data.gov team via email if you’re interested in using this service.
    > 
    > When an agency is ready for Data.gov to harvest its data.json for the first time, the agency should notify Data.gov via email and the Data.gov team will create a new Data.gov harvest source for the data.json. __The Data.gov team is available to assist agencies in generating the data.json file and provide tools that may help agencies prepare their data listings.__

- Cada conjunto de dados gerenciado pela DTA possui três representações:
    - Repositório git (`conjunto-dados/`)
    - Data Package (`conjunto-dados/datapackage.json`)
    - Dataset (`dataset/conjunto-dados`)

    A mesma slug (eg. `conjunto-dados`) deve ser utilizada como identificação nos três locais. As regras de estilo para determinação da slug estão no [issue #13](https://github.com/dados-mg/issues/issues/13).

- O processo de publicação de um conjunto de dados é de responsabilidade de cada órgão e entidade. Esse [blogpost](https://frictionlessdata.io/blog/2016/08/29/publish-online/) exemplifica a publicação com a máxima "Since a Data Package is just some files there are as many ways to do this as there are ways to put files online". O processo de catalogação de um conjunto de dados é uma responsabilidade da DTA/CGE. 

- A propriedade `path` de um recurso é [definida](https://specs.frictionlessdata.io/data-resource/#data-location) como uma _string_ “url-or-path”. Quando um data package possui recursos com o valor da propriedade `path` que referencia arquivos em disco, isso gera uma situação em que o data package não é completo somente com o arquivo `datapackage.json`. O processo de catalogação desses data packages por natureza é mais complexo.

- [Utilização do git LFS para versionamento de arquivos grandes #51](https://github.com/dados-mg/issues/issues/51)


- O serviço de validação do goodtables.io possui limites de computação que foram atingidos para o projeto da remuneração. Algumas alternativas são
    - Trazer a computação para a infraestrutura da CGE, usando, por exemplo, o [ckanext-validation](https://github.com/frictionlessdata/ckanext-validation)
    - Trazer a computação para uma infraestrutura de integração continua oferecida por serviços em nuvem. Vide exemplo da [etalab](https://github.com/etalab/tableschema-template). 
    - As duas alternativas não são necessariamente mutuamente excludentes, mas a última  é preferencial tendo em vista que os publicadores ainda podem fazer uso do processo de validação antes da catalogação no portal.

- Licenciamento de conjuntos de dados é um ponto de atenção que deve ser explorado no futuro. Para as questões de licenças, a [Open Data Commons](https://opendatacommons.org/) e o [Produto GT1: Levantamento Juridico sobre Licenciamento para Dados Abertos](https://wiki.dados.gov.br/Produto-GT1-Levantamento-Juridico-Licenciamento-Dados-Abertos.ashx) são pontos de introdução.

- Utilização de hospedagem de serviços em nuvem é um ponto de atenção. Nas [NORMAS PARA A UTILIZAÇÃO DOS RECURSOS TECNOLÓGICOS](https://www.tce.mg.gov.br/arquivosdoc/Anexo_Portaria_34.pdf#14) do TCE/MG, por exemplo, é vedado ao usuário

    > 2.10.15 É vedado ao usuário:
    > [...]
    >
    > e) A utilização de sites de serviços de armazenamento de informações em “nuvem” (Cloud Storage), tais como Dropbox, Googledrive, Onedrive, Skydrive, 4Shared, Wetransfer e afins;

    Serviços de gerenciamento de projetos e tarefas (eg. trello) e hospedagem de repositórios de software (eg. github), se enquadrariam nesse política?

- Visão: [working in the open](https://github.com/transparencia-mg/transparencia-mg.github.io) and [loving code](https://analise-viz-dados-master.github.io/slides/02_analise-dados#23)
  - [Why RStudio Focuses on Code-Based Data Science](https://blog.rstudio.com/2020/11/17/an-interview-with-lou-bajuk/)

    > One is to build your analyses with code, not clicks. Data science teams should use a code-oriented approach because code can be developed and adapted to solve similar problems in the future. This reusable and extensible code then becomes the core intellectual property for an organization. It'll make it easier over time to solve new problems and to increase the aggregate value of your data science work. This is why code-first data science is really a critical part of RStudio's philosophy and our roadmap.
    > 
    > The second major approach is to manage your data science environments for reproducibility. Organizations need a way to reproduce reports and dashboards as projects, tools, and dependencies change. You'll often hear about repeatability and reproducibility when talking about a heavily regulated environment like pharmaceuticals, and it's certainly particularly critical there. However, it's critical for every industry; otherwise your team may spend far too much time attempting to recreate old results. Worse, you may get different answers to the same questions at different points in time, which really undermines your team's credibility.
  - [Building a Mature Analytics Workflow](https://docs.getdbt.com/docs/about/viewpoint)

    > We believe that analytics teams have a workflow problem. Too often, analysts operate in isolation, and this creates suboptimal outcomes. Knowledge is siloed. We too often rewrite analyses that a colleague had already written. We fail to grasp the nuances of datasets that we’re less familiar with. We differ in our calculations of a shared metric.
    > 
    > We have convinced ourselves after hundreds of conversations that these conditions are by and large the status quo for even sophisticated analytics teams. As a result, organizations suffer from reduced decision speed and reduced decision quality.
    > 
    > Analytics doesn’t have to be this way. In fact, the playbook for solving these problems already exists — on our software engineering teams. The same techniques that software engineering teams use to collaborate on the rapid creation of quality applications can apply to analytics. We believe it’s time to build an open set of tools and processes to make that happen.
  - Ps. Nem todos conconcordam com essa visão, alguns contra-argumentos válidos [aqui](https://www.informationweek.com/software/information-management/kimball-university-should-you-use-an-etl-tool/d/d-id/1066486?) e [aqui](https://www.acunetix.com/blog/articles/source-code-disclosure-dangerous/) em áreas que estamos ativamente envolvidos.



## Encaminhamentos

- Solicitar configuração [letsencrypt](https://letsencrypt.org/pt-br/) para PdA e PdT (vide [issue #50](https://github.com/dados-mg/issues/issues/50))

## Material de Referência

- [Build tools - Makefile](http://swcarpentry.github.io/make-novice/)

    O `make` pode ser resumido como:

    > Make is a tool which can run commands to read files, process these files in some way, and write out the processed files. For example, in software development, Make is used to compile source code into executable programs or libraries, but Make can also be used to:
    > 
    > - run analysis scripts on raw data files to get data files that summarize the raw data;
    > - run visualization scripts on data files to produce plots; and to
    > - parse and combine text files and plots to create papers.
    >
    > Make is called a build tool - it builds data files, plots, papers, programs or libraries. It can also update existing files if desired.
    > 
    > Make tracks the dependencies between the files it creates and the files used to create these. If one of the original files (e.g. a data file) is changed, then Make knows to recreate, or update, the files that depend upon this file (e.g. a plot).
    > 
    > There are now many build tools available, all of which are based on the same concepts as Make.

    Em todos os conjuntos dados gerenciados pela DTA/CGE, como por exemplo [compras-emergenciais-covid-19](https://github.com/dados-mg/compras-emergenciais-covid-19), estamos utilizando o `make` como forma de documentação e como forma de garantir que uma execução completa do fluxo de publicação pode ser executada pela linha de comando e, portanto, é automatizável.

    Para determinar o que exatamente será executado utilizamos um arquivo `Makefile`. Em resumo

    > A simple makefile consists of "rules" with the following shape:
    > ```
    > target ... : dependencies ...
    >       command
    >       ...
    >       ...
    >```
    > A target is usually the name of a file that is generated by a program; examples of targets are executable or object files. A target can also be the name of an action to carry out, such as `clean' (see Phony Targets).
    > 
    > A dependency is a file that is used as input to create the target. A target often depends on several files.
    > 
    > A command is an action that make carries out. A rule may have more than one command, each on its own line.

- [Replicação e sincronização de um data package com CKAN](https://discuss.okfn.org/t/replicacao-e-sincronizacao-de-um-data-package-com-ckan/9830)

- [A collaborative community for sharing best practices for using GitHub in government](https://government.github.io/best-practices/)

- [schema.data.gouv.fr - An Open Data Schema Catalog for France](https://frictionlessdata.io/blog/2020/05/22/etalab-case-study-schemas-data-gouv-fr/)

- [Excalidraw](https://excalidraw.com/#room=f08c6013ebe03c68a025,t7MQ-hzyQhEYwc9sM7hTAw)

- [HackMD](https://github.com/hackmdio)